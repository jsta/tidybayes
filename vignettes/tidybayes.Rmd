---
title: "Using tidy data with Bayesian samplers"
author: "Matthew Kay"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    df_print: kable
vignette: >
  %\VignetteIndexEntry{Using Tidy Data with Bayesian Samplers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
<style type="text/css">
.kable-table table {
  margin-left: 0;
}
img {
  border: none;
}
</style>
  
  
## Introduction
  
This vignette introduces the `tidybayes` package, which facilitates the use of tidy data (one observation per row) with Bayesian samplers in R. This vignette is geared towards working with tidy data in general-purpose samplers like JAGS or Stan. For a similar introduction to the use of `tidybayes` with high-level modelling functions such as those in `brms` or `rstanarm`, see <code>[vignette("tidy-brms")](tidy-brms.html)</code> or <code>[vignette("tidy-rstanarm")](tidy-rstanarm.html)</code>.
  
The default output (and sometimes input) data formats of popular samplers like JAGS and Stan often don't quite conform to the ideal of [tidy data](http://dx.doi.org/10.18637/jss.v059.i10). For example, input formats might expect a list instead of a data frame, and for all variables to be encoded as numeric values (requiring translation of factors to numeric values and the creation of index variables to store the number of levels per factor or the number of observations in a data frame). Output formats will often be in matrix form (requiring conversion for use with libraries like ggplot), and will use numeric indices (requiring conversion back into factor level names if the you wish to make meaningfully-labelled plots or estimates). `tidybayes` automates all of these sorts of tasks.
  
### Philosophy
  
There are a few core ideas that run through the `tidybayes` API that should (hopefully) make it easy to use:
  
1. __Tidy data does not always mean all parameter names as values__. In contrast to the `ggmcmc` library (which translates sampler results into a data frame with a `Parameter` and `value` column), the `spread_samples` function in `tidybayes` produces in data frames where the columns are named after parameters and (in some cases) indices of those parameters, as automatically as possible and using a syntax as close to the same way you would refer to those variables in the sampler's language as possible. A similar function to `ggmcmc`'s approach is also provided in `gather_samples`, since sometimes you *do* want parameter names as values in a column. The goal is for `tidybayes` to do the tedious work of figuring out how to make a data frame look the way you need it to, including turning parameters with indices like `"b[1,2]"` and the like into tidy data for you.
  
2. __Fit into the tidyverse and broom__. `tidybayes` methods fit into a workflow familiar to users of the `tidyverse` (`dplyr`, `tidyr`, `ggplot2`, etc), and the `tidy` function from the `broom` package, which means fitting into the pipe (`%>%`) workflow, using and respecting grouped data frames (thus `spread_samples` and `gather_samples` return results already grouped by parameter indices, and methods like `mean_qi` calculate estimates and intervals for parameters and groups simultaneously), using the same output column names that the `broom::tidy` function does where possible, and not reinventing too much of the wheel if it is already made easy by functions provided by existing `tidyverse` packages (unless it makes for much clearer code for a common idiom).
  
3. __Focus on composable operations and plotting primitives, not monolithic plots and operations__. Several other packages (notably `bayesplot` and `ggmcmc`) already provide an excellent variety of pre-made methods for plotting Bayesian results. `tidybayes` shies away from duplicating this functionality. Instead, it focuses on providing composable operations for generating and manipulating Bayesian samples in a tidy data format, and graphical primitives for `ggplot` that allow you to build custom plots easily. Most simply, where `bayesplot` and `ggmcmc` tend to have functions with many options that return a full ggplot object, `tidybayes` tends towards providing primitives (like `geom`s) that you can compose and combine into your own custom plots. I believe both approaches have their place---monoliths can be useful for beginners---but I think that [composable operations tend to be more flexible and useful over the long term, and are more in the spirit of ggplot](http://blog.mjskay.com/2017/11/05/i-don-t-want-your-monolithic-ggplot-function/).
  
4. __Sensible defaults make life easy.__ But options (and the data being tidy in the first place) make it easy to go your own way when you need to.
  
5. __Variable names in models should be descriptive, not cryptic__. This principle implies avoiding cryptic (and short) subscripts in favor of longer (but descriptive) ones. This is a matter of readability and accessibility of models to others. For example, a common pattern amongst Stan users (and in the Stan manual) is to use variables like `J` to refer to the number of elements in a group (e.g., number of participants) and a corresponding index like `j` to refer to specific elements in that group. I believe this sacrifices too much readability for the sake of concision; I prefer a pattern like `n_participant` for the size of the group and `participant` (or a mnemonic shortform like `p`) for specific elements. In functions where names are auto-generated (like `compose_data`), `tidybayes` will (by default) assume you want these sorts of more descriptive names; however, you can always override the default naming scheme.
  

### Supported model types

`tidybayes` aims to support a variety of models. Currently supported models include [rstan](https://cran.r-project.org/package=rstan), [coda::mcmc and coda::mcmc.list](https://cran.r-project.org/package=coda), [runjags](https://cran.r-project.org/package=runjags), [rstanarm](https://cran.r-project.org/package=rstanarm), [brms](https://cran.r-project.org/package=brms), [MCMCglmm](https://cran.r-project.org/package=MCMCglmm), and anything with its own `as.mcmc.list` implementation. If you install the [tidybayes.rethinking](https://github.com/mjskay/tidybayes.rethinking) package, models from the [rethinking](https://github.com/rmcelreath/rethinking) package are also supported.

For an up-to-date list of supported models, see `?"tidybayes-models"`.


## Setup
  
The following libraries are required to run this vignette:
  
```{r setup, message = FALSE, warning = FALSE}
library(magrittr)
library(dplyr)
library(forcats)
library(ggplot2)
library(ggstance)
library(emmeans)
library(broom)
library(rstan)
library(rstanarm)
library(brms)
library(modelr)
library(bayesplot)
library(tidybayes)
```

These options help Stan run faster:

```{r, eval=FALSE}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

```{r hidden_options, include=FALSE}
# While the previous code chunk is the actual recommended approach,
# CRAN vignette building policy limits us to 2 cores, so we use at most
# 2 to build this vignette (but show the previous chunk to
# the reader as a best pratice example)
rstan_options(auto_write = TRUE)
options(mc.cores = min(2, parallel::detectCores()))

#ggplot options
theme_set(theme_light())

#figure options
knitr::opts_chunk$set(fig.width = 6, fig.height = 4)

options(width = 100)
```

## Example dataset

To demonstrate `tidybayes`, we will use a simple dataset with 10 observations from 5 conditions each:

```{r}
set.seed(5)
n = 10
n_condition = 5
ABC =
  data_frame(
    condition = rep(c("A","B","C","D","E"), n),
    response = rnorm(n * 5, c(0,1,2,1,-1), 0.5)
  )
```

A snapshot of the data looks like this:

```{r}
head(ABC, 10)
```
*(10 rows of `r nrow(ABC)`)*

This is a typical tidy format data frame: one observation per row. Graphically:

```{r}
ABC %>%
  ggplot(aes(x = response, y = fct_rev(condition))) +
  geom_point(alpha = 0.5) +
  ylab("condition")
```

## Using `compose_data` to prepare a data frame for the sampler

Shunting data from a data frame into a format usable in samplers like JAGS or Stan can involve a tedious set of operations, like generating index variables storing the number of operations or the number of levels in a factor. `compose_data` automates these operations.

A hierarchical model of our example data might estimate an overall mean across the conditions (`overall_mean`), the standard deviation of the condition means (`condition_mean_sd`), the mean within each condition (`condition_mean[condition]`) and the standard deviation of the responses given a condition mean (`response_sd`):

```{stan, output.var="ABC_stan"}
data {
  int<lower=1> n;
  int<lower=1> n_condition;
  int<lower=1, upper=n_condition> condition[n];
  real response[n];
}
parameters {
  real overall_mean;
  vector[n_condition] condition_zoffset;
  real<lower=0> response_sd;
  real<lower=0> condition_mean_sd;
}
transformed parameters {
  vector[n_condition] condition_mean;
  condition_mean = overall_mean + condition_zoffset * condition_mean_sd;
}
model {
  response_sd ~ cauchy(0, 1);         // => half-cauchy(0, 1)
  condition_mean_sd ~ cauchy(0, 1);   // => half-cauchy(0, 1)
  overall_mean ~ normal(0, 5);
  condition_zoffset ~ normal(0, 1);   // => condition_mean ~ normal(overall_mean, condition_mean_sd)
  for (i in 1:n) {
    response[i] ~ normal(condition_mean[condition[i]], response_sd);
  }
}
```

We have compiled and loaded this model into the variable `ABC_stan`.

This model expects these variables as input:

* `n`: number of observations
* `n_condition`: number of conditions
* `condition`: a vector of integers indicating the condition of each observation
* `response`: a vector of observations

Our data frame (`ABC`) only has `response` and `condition`, and `condition` is in the wrong format (it is a factor instead of numeric). However, `compose_data` can generate a list containing the above variables in the correct format automatically. It recognizes that `condition` is a factor and converts it to a numeric, adds the `n_condition` variable automatically containing the number of levels in `condition`, and adds the `n` column containing the number of observations (number of rows in the data frame):

```{r}
compose_data(ABC)
```

This makes it easy to skip right to running the model without munging the data yourself:

```{r}
m = sampling(ABC_stan, data = compose_data(ABC), control = list(adapt_delta=0.99))
```

The results look like this:

```{r}
print(m, pars = c("overall_mean", "condition_mean_sd", "condition_mean", "response_sd"))
```


## Extracting samples from a fit in tidy-format using `spread_samples`

### Extracting parameter indices into a separate column in a tidy format data frame

Now that we have our results, the fun begins: getting the samples out in a tidy format! The default methods in Stan for extracting samples from the data do so in a nested format:

```{r}
str(rstan::extract(m))
```

There are also methods for extracting samples as matrices or data frames in stan (and other model types, such as JAGS and MCMCglmm, have their own formats).

The `spread_samples` method yields a common format for all model types supported by `tidybayes`. It lets us instead extract samples into a data frame in tidy format, with a `.chain` and `.iteration` column storing the chain and iteration for each row, and the remaining columns corresponding to parameters or parameter indices. The `spread_samples` method accepts any number of column specifications, which can include names for parameters and names for parameter indices. For example, we can extract the `condition_mean` parameter as a tidy data frame, and put the value of its first (and only) index into the `condition` column, using a syntax that directly echoes how we would specify indices of the `condition_mean` parameter in the model itself:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  head(10)
```
*(10 rows of `r nrow(ABC)`)*

### Automatically converting columns and indices back into their original data types

As-is, the resulting parameters don't know anything about where their indices came from. The index of the `condition_mean` parameter was originally derived from the `condition` factor in the `ABC` data frame. But Stan doesn't know this: it is just a numeric index to Stan, so the `condition` column just contains numbers (`1, 2, 3, 4, 5`) instead of the factor levels these numbers correspond to (`"A", "B", "C", "D", "E"`).

We can recover this missing type information by passing the model through `recover_types` before using `spread_samples`. In itself `recover_types` just returns a copy of the model, with some additional attributes that store the type information from the data frame (or other objects) that you pass to it. This doesn't have any useful effect by itself, but functions like `spread_samples` use this information to convert any column or index back into the data type of the column with the same name in the original data frame. In this example, `spread_samples` recognizes that the `condition` column was a factor with five levels (`"A", "B", "C", "D", "E"`) in the original data frame, and automatically converts it back into a factor:

```{r}
m %>%
  recover_types(ABC) %>%
  spread_samples(condition_mean[condition]) %>%
  head(10)
```
*(10 rows of `r nrow(spread_samples(m, condition_mean[condition]))`)*

Because we often want to make multiple separate calls to `spread_samples`, it is often convenient to decorate the original model using `recover_types` immediately after it has been fit, so we only have to call it once:

```{r}
m %<>% recover_types(ABC)
```

Now we can omit the `recover_types` call before subsequent calls to `spread_samples`.

## Point estimates and intervals with the `point_interval` functions: `[mean|median|mode]_[qi|hdi]`

### With simple parameters, wide format

`tidybayes` provides a family of functions for generating point estimates and intervals from samples in a tidy format. These functions follow the naming scheme `[mean|median|mode]_[qi|hdi]`, for example, `mean_qi`, `median_qi`, `mode_hdi`, and so on. The first name (before the `_`) indicates the type of point estimate, and the second name indicates the type of interval. `qi` yields a quantile interval (a.k.a. equi-tailed interval, central interval, or percentile interval) and `hdi` yields a highest density interval. Custom estimates or intervals can also be applied using the `point_interval` function.

For example, we might extract the samples corresponding to the overall mean and standard deviation of observations:

```{r}
m %>%
  spread_samples(overall_mean, response_sd) %>%
  head(10)
```
*(10 rows of `r nrow(as_sample_tibble(m))`)*

Like with `condition_mean[condition]`, this gives us a tidy data frame. If we want the mean and 95% quantile interval of the parameters, we can apply `mean_qi`:

```{r}
m %>%
  spread_samples(overall_mean, response_sd) %>%
  mean_qi(overall_mean, response_sd)
```

`mean_qi` summarizes each input column using its mean. If there are multiple columns to summarise, each gets its own `x.high` and `x.low` column (for each column `x`) corresponding to the bounds of the `.prob`% interval. If there is only one column, the names `conf.low` and `conf.high` are used for the interval bounds; this is for compatibility with `broom::tidy`.

We can specify the columns we want to get means and intervals from, as above, or if we omit the list of columns, `mean_qi` will use every column that is not a grouping column or a special column (one that starts with `.`, like `.chain` or `.iteration`). Thus in the above example, `overall_mean` and `response_sd` are redundant arguments to `mean_qi` because they are also the only columns we gathered from the model. So we can simplify the previous code to the following:

```{r}
m %>%
  spread_samples(overall_mean, response_sd) %>%
  mean_qi()
```

### With indexed parameters

When we have a parameter with one or more indices, such as `condition_mean`, we can apply `mean_qi` (or other functions in the `point_estimate` family) as we did before:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  mean_qi()
```

How did `mean_qi` know what to aggregate? Data frames returned by `spread_samples` are automatically grouped by all index variables you pass to it; in this case, that means it groups by `condition`. `mean_qi` respects groups, and calculates the estimates and intervals within all groups. Then, because no columns were passed to `mean_qi`, it acts on the only non-special (`.`-prefixed) and non-group column, `condition_mean`. So the above shortened syntax is equivalent to this more verbose call:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  group_by(condition) %>%    # this line not necessary (done automatically by spread_samples)
  mean_qi(condition_mean)
```

When given only a single column, `mean_qi` will use the names `conf.low` and `conf.high` for the lower and upper ends of the intervals, in order to be consistent with `broom::tidy`.

## Plotting point estimates and intervals

### Using `geom_pointinterval`/`geom_pointintervalh`

Plotting means and intervals is straightforward using the `pointinterval` geom (or its horizontal version, `pointintervalh`), which are a modified versions of `ggplot2::geom_pointrange` and `ggstance::geom_pointrangeh` with sensible defaults for multiple intervals (functionality we will use later):

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  mean_qi() %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean, xmin = conf.low, xmax = conf.high)) +
  geom_pointintervalh()
```

`geom_pointintervalh` includes `xmin = conf.low` and `xmax = conf.high` in its default aesthetics, so these can be omitted:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  mean_qi() %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean)) +
  geom_pointintervalh()
```

### Using `stat_pointinterval`/`stat_pointintervalh`

Rather than summarizing the posterior before calling ggplot, we could also use `stat_pointinterval` / `stat_pointintervalh` to perform the summary within ggplot.

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean)) +
  stat_pointintervalh()
```

#### A note on functions ending in `h`

`tidybayes` follows the naming convention established by `ggstance` for horizontal versions of `geom`s and summary functions. It includes horizontal versions of all `point_interval` functions, which differ only in their behavior when passed a vector instead of a data frame. These functions have the same name as their vertical counterparts, but end with `h`. When calling `stat_pointintervalh` (the horizontal version of `stat_pointinterval`), we must use the horizontal versions of the `point_interval` functions, like `mean_qih` in the above example. This is because `mean_qi` returns a data frame with `y`, `ymin`, and `ymax` columns when passed a vector (making it suitable for `stat_pointinterval` or `stat_summary`), and `mean_qih` returns a data frame with `x`, `xmin`, and `xmax` columns when passed a vector (making it suitable for `stat_pointintervalh` or `stat_summaryh`).

### Interval estimates with posterior violins ("eye plots"): `geom_eye` and `geom_eyeh`

The `stat_summary` approach to generating intervals makes it easy to add violin plots of posterior densities to the plot using `geom_violin`/`geom_violinh`, forming "eye plots":

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean)) +
  geom_violinh(color = NA, fill = "gray65") +
  stat_pointintervalh(.prob = c(.95, .66))
```

The `geom_eye` and `geom_eyeh` geoms provide a shortcut to generating eye plots with some sensible defaults:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean)) +
  geom_eyeh()
```


### Interval estimates with posterior densities ("half-eye plots"): `geom_halfeyeh`

If you prefer densities over violins, you can use `geom_halfeyeh` (the vertical version, not yet available, will be called `geom_halfeye`). This example also demonstrates how to change the interval probability (here, to 99% and 80% intervals):

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean)) +
  geom_halfeyeh(.prob = c(.99, .8))
```


### Interval estimates with multiple probability levels: the `.prob =` argument

`mean_qi` and its sister functions can also produce an arbitrary number of probability intervals by setting the `.prob =` argument:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  mean_qi(.prob = c(.95, .8, .5))
```

The results are in a tidy format: one row per index (`condition`) and probability level (`.prob`). This facilitates plotting. For example, assigning `-.prob` to the `size` aesthetic will show all intervals, making thicker lines correspond to smaller intervals:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  mean_qi(.prob = c(.95, .66)) %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean, 
    size = -.prob)) +               # smaller probability interval => thicker line
  geom_pointintervalh()
```

Just as `geom_pointintervalh` includes `xmin = conf.low` and `xmax = conf.high` as default aesthetics, it also includes 
`size = -.prob` to facilitate exactly this usage. This, the above can be simplified to:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  mean_qi(.prob = c(.95, .66)) %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean)) +
  geom_pointintervalh()
```

Just as the `point_interval` functions can generate an arbitrary number of intervals per estimate, so too can `geom_pointrangeh` draw an arbitrary number of intervals, though in most cases this starts to get pretty silly (and will require the use of `size_range =`, which determines the minimum and maximum line thickness, to make it legible). Here it is with 3:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  mean_qi(.prob = c(.95, .8, .5)) %>%
  ggplot(aes(y = fct_rev(condition), x = condition_mean)) +
  geom_pointintervalh(size_range = c(0.5, 2))
```


### Plotting posteriors as quantile dotplots

Intervals are nice if the alpha level happens to line up with whatever decision you are trying to make, but getting a shape of the posterior is better (hence eye plots, above). On the other hand, making inferences from density plots is imprecise (estimating the area of one shape as a proportion of another is a hard perceptual task). Reasoning about probability in frequency formats is easier, motivating [quantile dotplots](https://github.com/mjskay/when-ish-is-my-bus/blob/master/quantile-dotplots.md), which also allow precise estimation of arbitrary intervals (down to the dot resolution of the plot, here 100):

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  do(data_frame(condition_mean = quantile(.$condition_mean, ppoints(100)))) %>%
  ggplot(aes(x = condition_mean)) +
  geom_dotplot(binwidth = .04) +
  facet_grid(fct_rev(condition) ~ .) +
  scale_y_continuous(breaks = NULL)
```

The idea is to get away from thinking about the posterior as indicating one canonical point or interval, but instead to represent it as (say) 100 approximately equally likely points.


### Alternative estimates and intervals: mean, median, mode; qi, hdi

The `point_interval` family of functions follow the naming scheme `[mean|median|mode]_[qi|hdi][h|]`, and all work in the same way as `mean_qi`: they take a series of names (or expressions calculated on columns) and summarize those columns with the corresponding point estimate (mean, median, or mode) and interval (qi or hdi). `qi` yields a quantile interval (a.k.a. equi-tailed interval, central interval, or percentile interval) and `hdi` yields a highest (posterior) density interval. These can be used in any combination desired. 

The `*_hdi` functions have an additional difference: In the case of multimodal distributions, they may return multiple intervals for each probability level. Here are some samples from a multimodal normal mixture:

```{r}
set.seed(123)
multimodal_samples = data_frame(
  x = c(rnorm(5000, 0, 1), rnorm(2500, 4, 1))
)
```

Passed through `mode_hdi`, we get multiple intervals at the 80% probability level:

```{r}
multimodal_samples %>%
  mode_hdi(x, .prob = .80)
```

This is easier to see when plotted:

```{r, fig.height = 3, fig.width = 8}
multimodal_samples %>%
  ggplot(aes(x = x)) +
  stat_density(fill = "gray75") +
  stat_pointintervalh(aes(y = -0.05), fun.data = median_qih, .prob = c(.95, .80)) +
  annotate("text", label = "median, 80% and 95% quantile intervals", x = 6, y = -0.05, hjust = 0, vjust = 0.3) +
  stat_pointintervalh(aes(y = -0.025), fun.data = mode_hdih, .prob = c(.95, .80)) +
  annotate("text", label = "mode, 80% and 95% highest-density intervals", x = 6, y = -0.025, hjust = 0, vjust = 0.3) +
  xlim(-3.5, 14.5)
```


## Combining variables with different indices in a single tidy format data frame

`spread_samples` supports gathering variables that have different indices. It automatically matches up indices with the same name, and duplicates values as necessary to produce one row per all combination of levels of all indices. For example, we might want to calculate the difference between each condition mean and the overall mean. To do that, we can gather samples from the overall mean and all condition means:

```{r}
m %>% 
  spread_samples(overall_mean, condition_mean[condition]) %>%
  head(10)
```
*(10 rows of `r nrow(spread_samples(m, condition_mean[condition]))`)*

Within each sample, `overall_mean` is repeated as necessary to correspond to every index of `condition_mean`. Thus, the `mutate` function from dplyr can be used to take the differences over all rows, then we can summarize with `mean_qi`:

```{r}
m %>%
  spread_samples(overall_mean, condition_mean[condition]) %>%
  mutate(condition_offset = condition_mean - overall_mean) %>%
  mean_qi(condition_offset)
```

## Posterior predictions

We can use combinations of variables with difference indices to generate predictions from the model. In this case, we can combine the condition means with the residual standard deviation to generate predictive distributions from the model:

```{r}
m %>%
  spread_samples(condition_mean[condition], response_sd) %>%
  mutate(y_rep = rnorm(n(), condition_mean, response_sd)) %>%
  ggplot(aes(x = y_rep)) +
  stat_density() +
  facet_grid(condition ~ ., switch = "y")
```

And even summarize these as predictive intervals and compare them to the data:

```{r}
m %>%
  spread_samples(condition_mean[condition], response_sd) %>%
  mutate(y_rep = rnorm(n(), condition_mean, response_sd)) %>%
  mean_qi(y_rep, .prob = c(.95, .8, .5)) %>%
  ggplot(aes(y = fct_rev(condition), x = y_rep)) +
  geom_intervalh() + #auto-sets aes(xmin = conf.low, xmax = conf.high, color = fct_rev(ordered(.prob)))
  geom_point(aes(x = response), data = ABC) +
  scale_color_brewer()
```

If this model is well-calibrated, about 95% of the data should be within the outer intervals, 80% in the next-smallest intervals, and 50% in the smallest intervals.


### Posterior predictions with parameter estimates

Altogether, data, posterior predictions, and estimates of the means:

```{r}
samples = m %>%
  spread_samples(condition_mean[condition], response_sd)

reps = samples %>%
  mutate(y_rep = rnorm(n(), condition_mean, response_sd)) %>%
  mean_qi(y_rep, .prob = c(.95, .8, .5))

parameters = samples %>%
  mean_qi(condition_mean, .prob = c(.95, .66))

ABC %>%
  ggplot(aes(y = condition)) +
  geom_intervalh(aes(x = y_rep), data = reps) +
  geom_pointintervalh(aes(x = condition_mean), position = position_nudge(y = -0.2), data = parameters) +
  geom_point(aes(x = response)) +
  scale_color_brewer()
```

## Comparing levels of a factor

`compare_levels` allows us to compare the value of some variable across levels of some factor. By default it computes all pairwise differences, though this can be changed using the `comparison = ` parameter:

```{r, fig.width=7}
#N.B. the syntax for compare_levels is experimental and may change
m %>%
  spread_samples(condition_mean[condition]) %>%
  compare_levels(condition_mean, by = condition) %>%
  ggplot(aes(y = condition, x = condition_mean)) +
  geom_halfeyeh()
```


## Gathering all parameter names into a single column: `gather_samples` and `gather_terms`

We might also prefer all parameter names to be in a single column (long-format) instead of as column names. There are two methods for obtaining long-format data frames with `tidybayes`, whose use depends on where and how in the data processing chain you might want to transform into long-format: `gather_samples` and `gather_terms`. There are also two methods for wide (or semi-wide) format data frame, `spread_samples` (described previously) and `as_sample_tibble`.

`gather_samples` is the counterpart to `spread_samples`, except it puts all parameter names into a `term` column and samples into an `estimate` column (for compatibility with `broom::tidy`):

```{r}
m %>%
  gather_samples(overall_mean, condition_mean[condition]) %>%
  mean_qi()
```

Note that `condition = NA` for the `overall_mean` row, because it does not have an index with that name in the specification passed to `gather_samples`.

While this works well if we do not need to perform computations that involve multiple columns, the semi-wide format returned by `spread_samples` is very useful for computations that involve multiple columns names, such as the calculation of the `condition_offset` above. If we want to make intermediate computations on the format returned by `spread_samples` and *then* gather parameters into one column, we can use `gather_terms`, which will gather all non-grouped terms that do not start with `"."`:

```{r}
m %>%
  spread_samples(overall_mean, condition_mean[condition]) %>%
  mutate(condition_offset = condition_mean - overall_mean) %>%
  gather_terms() %>%
  mean_qi()
```

Note how `overall_mean` is now repeated here for each condition, because we have performed the gather after spreading parameters across columns.

Finally, if we want raw parameter names as columns names instead of having indices split out as their own column names, we can use `as_sample_tibble`. Generally speaking this should not be necessary, but is provided as a common method for generating data frames from many types of Bayesian models, and is used internally by `gather_samples` and `spread_samples`:

```{r}
m %>%
  as_sample_tibble() %>%
  head(10)
```
*(10 rows of `r nrow(as_sample_tibble(m))`)*

Combining `as_sample_tibble` with `gather_terms` also allows us to derive similar output to `ggmcmc::ggs`, if desired:

```{r}
m %>%
  as_sample_tibble() %>%
  gather_terms() %>%
  head(10)
```
*(10 rows of `r nrow(gather_terms(as_sample_tibble(m)))`)*

But again, this approach does not handle parameter indices for us automatically, so using `spread_samples` and `gather_samples` is generally recommended unless you do not have parameter indices to worry about.


## Selecting parameters using regular expressions

You can use regular expressions in the specifications passed to `spread_samples` and `gather_samples` to match multiple columns by passing `regex = TRUE`. Our example fit contains parameters named `condition_mean[i]` and `condition_zoffset[i]`. We could extract both using a single regular expression:

```{r}
m %>%
  spread_samples(`condition_.*`[condition], regex = TRUE) %>%
  head(10)
```
*(10 rows of `r nrow(spread_samples(m, condition_mean[condition]))`)*

This result is equivalent in this case to `spread_samples(c(condition_mean, condition_zoffset)[condition])`, but does not require us to list each parameter explicitly---this can be useful, for example, in models with naming schemes like `b_[some name]` for coefficients.


## Drawing fit curves with uncertainty

To demonstrate drawing fit curves with uncertainty, let's fit a slightly naive model to part of the `mtcars` dataset using `brm`:

```{r, results = "hide", message = FALSE, warning = FALSE}
m_mpg = brm(mpg ~ hp * cyl, data = mtcars)
```

We can draw fit curves with probability bands using `tidybayes::add_fitted_samples` and `tidybayes::stat_lineribbon`:

```{r}
mtcars %>%
  group_by(cyl) %>%
  data_grid(hp = seq_range(hp, n = 101)) %>%
  add_fitted_samples(m_mpg) %>%
  ggplot(aes(x = hp, y = mpg, color = ordered(cyl))) +
  stat_lineribbon(aes(y = estimate)) +
  geom_point(data = mtcars) +
  scale_fill_brewer(palette = "Greys")
```

Or we can sample a reasonable number of fit lines (say 100) and overplot them:

```{r}
mtcars %>%
  group_by(cyl) %>%
  data_grid(hp = seq_range(hp, n = 101)) %>%
  add_fitted_samples(m_mpg, n = 100) %>%
  ggplot(aes(x = hp, y = mpg, color = ordered(cyl))) +
  geom_line(aes(y = estimate, group = paste(cyl, .iteration)), alpha = 0.25) +
  geom_point(data = mtcars)
```

For more exmples of curve-drawing (including posterior predictions), see `vignette("tidy-brms")` or `vignette("tidy-rstanarm")`.


## Compatibility with other packages

### Compatibility of `point_interval` with `broom::tidy` and `dotwhisker::dwplot`: A model comparison example

Because `mean_qi` and the `point_interval` family of functions use a similar naming scheme to that of `broom::tidy`, it is easy to compare results against models supported by `broom::tidy`. For example, let's compare our model's estimates of conditional means against an ordinary least squares (OLS) regression:

```{r}
m_linear = lm(response ~ condition, data = ABC)
```

Combining `emmeans::emmeans` with `broom::tidy`, we can generate tidy-format estimates of conditional means from the above model:

```{r}
linear_estimates = m_linear %>% 
  emmeans(~ condition) %>% 
  tidy() %>%
  mutate(model = "OLS")

linear_estimates
```

We can derive corresponding estimates from our model:

```{r}
bayes_estimates = m %>%
  spread_samples(condition_mean[condition]) %>%
  mean_qi(estimate = condition_mean) %>%
  mutate(model = "Bayes")

bayes_estimates
```

Because the `point_interval` functions use the same column names for `conf.low` and `conf.high` as `broom::tidy` does, the column names for the columns we need to make the comparison (`condition`, `estimate`, `conf.low`, and `conf.high`) all line up easily. This makes it simple to combine the two tidy data frame together using `bind_rows`, and plot them:

```{r}
bind_rows(linear_estimates, bayes_estimates) %>%
  mutate(condition = fct_rev(condition)) %>%
  ggplot(aes(y = condition, x = estimate, xmin = conf.low, xmax = conf.high, color = model)) +
  geom_pointrangeh(position = position_dodgev(height = .3))
```

Comptability with `tidy` also gives compatibility with `dotwhisker::dwplot`:

```{r, warning = FALSE}
bind_rows(linear_estimates, bayes_estimates) %>%
    rename(term = condition) %>%
    dotwhisker::dwplot()
```

Observe the shrinkage towards the overall mean in the Bayesian model compared to the OLS model.

### Compatibility with `bayesplot` and other non-tidy packages using `unspread_samples` and `ungather_samples`

Functions from other packages might expect samples in the form of a data frame or matrix with parameters as columns and samples as rows. That is the format returned by `as_sample_tibble`, but not by `gather_samples` or `spread_samples`, which split indices from parameters out into columns. 

It may be desirable to use the `spread_samples` or `gather_samples` functions to transform your samples in some way, and then convert them *back* into the sample $\times$ parameter format to pass them into functions from other packages, like `bayesplot`. The `unspread_samples` and `ungather_samples` functions invert `spread_samples` and `gather_samples` to return a data frame with parameter column names that include indices in them and samples as rows.

As an example, let's re-do the previous example of `compare_levels`, but use `bayesplot::mcmc_hist` to plot the results instead of `geom_eyeh`. First, the result of `compare_levels` looks like this:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  compare_levels(condition_mean, by = condition) %>%
  head(10)
```
*(10 rows of `r nrow(m %>% spread_samples(condition_mean[condition]) %>% compare_levels(condition_mean, by = condition))`)*

To get a version we can pass to `mcmc_hist`, all we need to do is invert the `spread_samples` call we started with:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  compare_levels(condition_mean, by = condition) %>%
  unspread_samples(condition_mean[condition]) %>%
  head(10)
```
*(10 rows of `r nrow(as_sample_tibble(m))`)*

We can pass that into `bayesplot::mcmc_areas` directly. The `drop_indices = TRUE` parameter to `unspread_samples` indicates that `.chain` and `.iteration` should not be included in the output:

```{r}
m %>%
  spread_samples(condition_mean[condition]) %>%
  compare_levels(condition_mean, by = condition) %>%
  unspread_samples(condition_mean[condition], drop_indices = TRUE) %>%
  mcmc_areas()
```

If you are instead working with tidy samples generated by `gather_samples` or `gather_terms`, the `ungather_samples` function will transform those samples into the sample $\times$ parameter format. It has the same syntax as `unspread_samples`.


### Compatibility with `emmeans` (formerly `lsmeans`)

The `emmeans` package provides a convenient syntax for generating marginal estimates from a model, including numerous types of contrasts. It also supports some Bayesian modeling packages, like `MCMCglmm` and `rstanarm`. However, it does not provide samples in a tidy format. The `gather_emmeans_samples` function converts output from `emmeans` into a tidy format, keeping the `emmeans` reference grid and adding an `estimate` column with long-format samples.

For example, given this `rstanarm` model:

```{r}
m_rst = stan_glm(response ~ condition, data = ABC)
```

We can use `emmeans::emmeans` to get conditional means with uncertainty:

```{r}
m_rst %>%
  emmeans( ~ condition) %>%
  gather_emmeans_samples() %>%
  mean_qi()
```

Or `emmeans::emmeans` with `emmeans::contrast` to do all pairwise comparisons:

```{r}
m_rst %>%
  emmeans( ~ condition) %>%
  contrast(method = "pairwise") %>%
  gather_emmeans_samples() %>%
  mean_qi()
```

See `?pairwise.emmc` for a list of the numerous contrast types supported by `emmeans`.

As before, we can plot the results instead of using a table:

```{r}
m_rst %>%
  emmeans( ~ condition) %>%
  contrast(method = "pairwise") %>%
  gather_emmeans_samples() %>%
  ggplot(aes(x = estimate, y = contrast)) +
  geom_halfeyeh()
```
